{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "349b1dd6-1c4e-4798-9856-28cfc6213467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\유승미\\AppData\\Local\\Temp\\ipykernel_3740\\4225343611.py:5: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리를 불러들임 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd718ee-c1b0-47c8-b0e5-3600886c9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터 불러오기\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "545919aa-3367-4d0c-a82b-fd8b76893a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습관련 매개변수 설정\n",
    "n_input      = 784\n",
    "n_hidden1    = 400 # 1층 = 400\n",
    "n_hidden2    = 256 # 2층 = 256\n",
    "n_hidden3    = 128 # 2층 = 128\n",
    "n_hidden4    = 50 # 4층 =50\n",
    "display_step = 1\n",
    "n_epoch = 200 \n",
    "batch_size = 256 \n",
    "lr_rbm = tf.constant(0.001, tf.float32)\n",
    "lr_class = tf.constant(0.01, tf.float32)\n",
    "n_class = 10\n",
    "n_iter = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f49c863-72e3-4397-b97a-a1220267838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 및 출력을 정의함\n",
    "x  = tf.placeholder(tf.float32, [None, n_input], name=\"x\") \n",
    "y  = tf.placeholder(tf.float32, [None,10], name=\"y\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3eecb9-2fd4-4c36-8c3e-3c24464c7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 은닉층 관련 가중치 및 편향을 정의함\n",
    "W1  = tf.Variable(tf.random_normal([n_input, n_hidden1], 0.01), name=\"W1\") \n",
    "b1_h = tf.Variable(tf.zeros([1, n_hidden1],  tf.float32, name=\"b1_h\")) \n",
    "b1_i = tf.Variable(tf.zeros([1, n_input],  tf.float32, name=\"b1_i\")) \n",
    "\n",
    "# 두 번째 은닉층 관련 가중치 및 편향을 정의함\n",
    "W2  = tf.Variable(tf.random_normal([n_hidden1, n_hidden2], 0.01), name=\"W2\") \n",
    "b2_h = tf.Variable(tf.zeros([1, n_hidden2],  tf.float32, name=\"b2_h\")) \n",
    "b2_i = tf.Variable(tf.zeros([1, n_hidden1],  tf.float32, name=\"b2_i\")) \n",
    "\n",
    "# 세 번째 은닉층 관련 가중치 및 편향을 정의함\n",
    "W3  = tf.Variable(tf.random_normal([n_hidden2, n_hidden3], 0.01), name=\"W3\") \n",
    "b3_h = tf.Variable(tf.zeros([1, n_hidden3],  tf.float32, name=\"b3_h\")) \n",
    "b3_i = tf.Variable(tf.zeros([1, n_hidden2],  tf.float32, name=\"b3_i\")) \n",
    "\n",
    "# 네 번째 은닉층 관련 가중치 및 편향을 정의함\n",
    "W4  = tf.Variable(tf.random_normal([n_hidden3, n_hidden4], 0.01), name=\"W4\") \n",
    "b4_h = tf.Variable(tf.zeros([1, n_hidden4],  tf.float32, name=\"b4_h\")) \n",
    "b4_i = tf.Variable(tf.zeros([1, n_hidden3],  tf.float32, name=\"b4_i\")) \n",
    "\n",
    "# 라벨층 관련 가중치 및 편향을 정의함\n",
    "W_c = tf.Variable(tf.random_normal([n_hidden4,n_class], 0.01), name=\"W_c\") \n",
    "b_c = tf.Variable(tf.zeros([1, n_class],  tf.float32, name=\"b_c\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a621cc9a-2135-453e-9c09-f16260118d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률을 이산 상태, 즉 0과 1로 변환함 \n",
    "def binary(prob):\n",
    "    return tf.floor(prob + tf.random_uniform(tf.shape(prob), 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a203a7a-8b10-41d5-84ea-2e97a7ef97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibbs 표본추출 단계\n",
    "def cd_step(x_k,W,b_h,b_i):\n",
    "    h_k = binary(tf.sigmoid(tf.matmul(x_k, W) + b_h)) \n",
    "    x_k = binary(tf.sigmoid(tf.matmul(h_k, tf.transpose(W)) + b_i))\n",
    "    return x_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e866fcd-58db-4a4e-b737-61d80568cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표본추출 단계 실행     \n",
    "def cd_gibbs(k,x_k,W,b_h,b_i):\n",
    "    for i in range(k):\n",
    "        x_out = cd_step(x_k,W,b_h,b_i) \n",
    "    # k 반복 후에 깁스 표본을 반환함\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f15c8-557f-4bf3-a89c-490b9910c475",
   "metadata": {},
   "source": [
    "### 4개의 은닉층을 갖는 DBN에 대한 CD-2 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5f6acf-3cd4-4e8d-aee8-000d0a250f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개의 은닉층을 갖는 DBN에 대한 CD-2 알고리즘\n",
    "# 1. 현재 입력값을 기반으로 깁스 표본추출을 통해 새로운 입력값 x_s를 구함\n",
    "# 2. 새로운 x_s를 기반으로 새로운 은닉노드 값 h_s를 구함    \n",
    "x_s = cd_gibbs(2,x,W1,b1_h,b1_i) \n",
    "act_h1_s = binary(tf.sigmoid(tf.matmul(x_s, W1) + b1_h)) \n",
    "h1_s = cd_gibbs(2,act_h1_s,W2,b2_h,b2_i) \n",
    "act_h2_s = binary(tf.sigmoid(tf.matmul(h1_s, W2) + b2_h)) \n",
    "h2_s = cd_gibbs(2, act_h2_s, W3, b3_h, b3_i)  \n",
    "act_h3_s = binary(tf.sigmoid(tf.matmul(h2_s, W3) + b3_h))\n",
    "h3_s = cd_gibbs(2, act_h3_s, W4, b4_h, b4_i)  \n",
    "act_h4_s = binary(tf.sigmoid(tf.matmul(h3_s, W4) + b4_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb9bda45-b14d-4be4-99f6-55d643a78be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값이 주어질 때 은닉노드 값 h를 구함\n",
    "act_h1 = tf.sigmoid(tf.matmul(x, W1) + b1_h) \n",
    "act_h2 = tf.sigmoid(tf.matmul(act_h1_s, W2) + b2_h)\n",
    "act_h3 = tf.sigmoid(tf.matmul(act_h2_s, W3) + b3_h) \n",
    "act_h4 = tf.sigmoid(tf.matmul(act_h3_s, W4) + b4_h) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b54709ae-0bc5-4ef4-9e4d-e91dfd103bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법을 이용한 가중치 및 편향 업데이트 \n",
    "size_batch = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "# 첫 번째 은닉층\n",
    "W1_add  = tf.multiply(lr_rbm/size_batch, tf.subtract(tf.matmul(tf.transpose(x), \\\n",
    "          act_h1), tf.matmul(tf.transpose(x_s), act_h1_s)))\n",
    "b1_i_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(x, x_s), \\\n",
    "           0, True))\n",
    "b1_h_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h1, act_h1_s), \\\n",
    "           0, True))\n",
    "\n",
    "# 두 번째 은닉층\n",
    "W2_add  = tf.multiply(lr_rbm/size_batch, tf.subtract(tf.matmul(tf.transpose(act_h1_s), \\\n",
    "          act_h2), tf.matmul(tf.transpose(h1_s), act_h2_s)))\n",
    "b2_i_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h1_s, h1_s), \\\n",
    "           0, True))\n",
    "b2_h_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h2, act_h2_s), \\\n",
    "        0, True))\n",
    "\n",
    "# 세 번째 은닉층\n",
    "W3_add  = tf.multiply(lr_rbm/size_batch, tf.subtract(tf.matmul(tf.transpose(act_h2_s), \\\n",
    "          act_h3), tf.matmul(tf.transpose(h2_s), act_h3_s)))  \n",
    "b3_i_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h2_s, h2_s), \\\n",
    "           0, True))  \n",
    "b3_h_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h3, act_h3_s), \\\n",
    "         0, True))  \n",
    "\n",
    "# 네 번째 은닉층\n",
    "W4_add  = tf.multiply(lr_rbm/size_batch, tf.subtract(tf.matmul(tf.transpose(act_h3_s), \\\n",
    "          act_h4), tf.matmul(tf.transpose(h3_s), act_h4_s)))  \n",
    "b4_i_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h3_s, h3_s), \\\n",
    "           0, True))  \n",
    "b4_h_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h4, act_h4_s), \\\n",
    "         0, True))  \n",
    "\n",
    "updt = [W1.assign_add(W1_add), b1_i.assign_add(b1_i_add), b1_h.assign_add(b1_h_add),\\\n",
    "        W2.assign_add(W2_add), b2_i.assign_add(b2_i_add), b2_h.assign_add(b2_h_add),\\\n",
    "        W3.assign_add(W3_add), b3_i.assign_add(b3_i_add), b3_h.assign_add(b3_h_add),\\\n",
    "        W4.assign_add(W4_add), b4_i.assign_add(b4_i_add), b4_h.assign_add(b4_h_add)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15dfe48b-7b1b-41e7-85cb-9b93ecdf43a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\유승미\\AppData\\Local\\Temp\\ipykernel_19244\\102771575.py:7: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------\n",
    "# 소프트맥스 층을 추가한 분류용-DBN 을 위한 연산과정\n",
    "#------------------------------------------------------------- \n",
    "\n",
    "logits = tf.matmul(act_h4,W_c) + b_c\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr_class).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea766427-cbeb-48db-a8e5-eaf74585b51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\n",
      "Epoch: 0002\n",
      "Epoch: 0003\n",
      "Epoch: 0004\n",
      "Epoch: 0005\n",
      "Epoch: 0006\n",
      "Epoch: 0007\n",
      "Epoch: 0008\n",
      "Epoch: 0009\n",
      "Epoch: 0010\n",
      "Epoch: 0011\n",
      "Epoch: 0012\n",
      "Epoch: 0013\n",
      "Epoch: 0014\n",
      "Epoch: 0015\n",
      "Epoch: 0016\n",
      "Epoch: 0017\n",
      "Epoch: 0018\n",
      "Epoch: 0019\n",
      "Epoch: 0020\n",
      "Epoch: 0021\n",
      "Epoch: 0022\n",
      "Epoch: 0023\n",
      "Epoch: 0024\n",
      "Epoch: 0025\n",
      "Epoch: 0026\n",
      "Epoch: 0027\n",
      "Epoch: 0028\n",
      "Epoch: 0029\n",
      "Epoch: 0030\n",
      "Epoch: 0031\n",
      "Epoch: 0032\n",
      "Epoch: 0033\n",
      "Epoch: 0034\n",
      "Epoch: 0035\n",
      "Epoch: 0036\n",
      "Epoch: 0037\n",
      "Epoch: 0038\n",
      "Epoch: 0039\n",
      "Epoch: 0040\n",
      "Epoch: 0041\n",
      "Epoch: 0042\n",
      "Epoch: 0043\n",
      "Epoch: 0044\n",
      "Epoch: 0045\n",
      "Epoch: 0046\n",
      "Epoch: 0047\n",
      "Epoch: 0048\n",
      "Epoch: 0049\n",
      "Epoch: 0050\n",
      "Epoch: 0051\n",
      "Epoch: 0052\n",
      "Epoch: 0053\n",
      "Epoch: 0054\n",
      "Epoch: 0055\n",
      "Epoch: 0056\n",
      "Epoch: 0057\n",
      "Epoch: 0058\n",
      "Epoch: 0059\n",
      "Epoch: 0060\n",
      "Epoch: 0061\n",
      "Epoch: 0062\n",
      "Epoch: 0063\n",
      "Epoch: 0064\n",
      "Epoch: 0065\n",
      "Epoch: 0066\n",
      "Epoch: 0067\n",
      "Epoch: 0068\n",
      "Epoch: 0069\n",
      "Epoch: 0070\n",
      "Epoch: 0071\n",
      "Epoch: 0072\n",
      "Epoch: 0073\n",
      "Epoch: 0074\n",
      "Epoch: 0075\n",
      "Epoch: 0076\n",
      "Epoch: 0077\n",
      "Epoch: 0078\n",
      "Epoch: 0079\n",
      "Epoch: 0080\n",
      "Epoch: 0081\n",
      "Epoch: 0082\n",
      "Epoch: 0083\n",
      "Epoch: 0084\n",
      "Epoch: 0085\n",
      "Epoch: 0086\n",
      "Epoch: 0087\n",
      "Epoch: 0088\n",
      "Epoch: 0089\n",
      "Epoch: 0090\n",
      "Epoch: 0091\n",
      "Epoch: 0092\n",
      "Epoch: 0093\n",
      "Epoch: 0094\n",
      "Epoch: 0095\n",
      "Epoch: 0096\n",
      "Epoch: 0097\n",
      "Epoch: 0098\n",
      "Epoch: 0099\n",
      "Epoch: 0100\n",
      "Epoch: 0101\n",
      "Epoch: 0102\n",
      "Epoch: 0103\n",
      "Epoch: 0104\n",
      "Epoch: 0105\n",
      "Epoch: 0106\n",
      "Epoch: 0107\n",
      "Epoch: 0108\n",
      "Epoch: 0109\n",
      "Epoch: 0110\n",
      "Epoch: 0111\n",
      "Epoch: 0112\n",
      "Epoch: 0113\n",
      "Epoch: 0114\n",
      "Epoch: 0115\n",
      "Epoch: 0116\n",
      "Epoch: 0117\n",
      "Epoch: 0118\n",
      "Epoch: 0119\n",
      "Epoch: 0120\n",
      "Epoch: 0121\n",
      "Epoch: 0122\n",
      "Epoch: 0123\n",
      "Epoch: 0124\n",
      "Epoch: 0125\n",
      "Epoch: 0126\n",
      "Epoch: 0127\n",
      "Epoch: 0128\n",
      "Epoch: 0129\n",
      "Epoch: 0130\n",
      "Epoch: 0131\n",
      "Epoch: 0132\n",
      "Epoch: 0133\n",
      "Epoch: 0134\n",
      "Epoch: 0135\n",
      "Epoch: 0136\n",
      "Epoch: 0137\n",
      "Epoch: 0138\n",
      "Epoch: 0139\n",
      "Epoch: 0140\n",
      "Epoch: 0141\n",
      "Epoch: 0142\n",
      "Epoch: 0143\n",
      "Epoch: 0144\n",
      "Epoch: 0145\n",
      "Epoch: 0146\n",
      "Epoch: 0147\n",
      "Epoch: 0148\n",
      "Epoch: 0149\n",
      "Epoch: 0150\n",
      "Epoch: 0151\n",
      "Epoch: 0152\n",
      "Epoch: 0153\n",
      "Epoch: 0154\n",
      "Epoch: 0155\n",
      "Epoch: 0156\n",
      "Epoch: 0157\n",
      "Epoch: 0158\n",
      "Epoch: 0159\n",
      "Epoch: 0160\n",
      "Epoch: 0161\n",
      "Epoch: 0162\n",
      "Epoch: 0163\n",
      "Epoch: 0164\n",
      "Epoch: 0165\n",
      "Epoch: 0166\n",
      "Epoch: 0167\n",
      "Epoch: 0168\n",
      "Epoch: 0169\n",
      "Epoch: 0170\n",
      "Epoch: 0171\n",
      "Epoch: 0172\n",
      "Epoch: 0173\n",
      "Epoch: 0174\n",
      "Epoch: 0175\n",
      "Epoch: 0176\n",
      "Epoch: 0177\n",
      "Epoch: 0178\n",
      "Epoch: 0179\n",
      "Epoch: 0180\n",
      "Epoch: 0181\n",
      "Epoch: 0182\n",
      "Epoch: 0183\n",
      "Epoch: 0184\n",
      "Epoch: 0185\n",
      "Epoch: 0186\n",
      "Epoch: 0187\n",
      "Epoch: 0188\n",
      "Epoch: 0189\n",
      "Epoch: 0190\n",
      "Epoch: 0191\n",
      "Epoch: 0192\n",
      "Epoch: 0193\n",
      "Epoch: 0194\n",
      "Epoch: 0195\n",
      "Epoch: 0196\n",
      "Epoch: 0197\n",
      "Epoch: 0198\n",
      "Epoch: 0199\n",
      "Epoch: 0200\n",
      "RBM training Completed !\n",
      "Iter 0, Minibatch Loss= 6.736001, Training Accuracy= 0.12891\n",
      "Iter 10, Minibatch Loss= 3.229908, Training Accuracy= 0.16406\n",
      "Iter 20, Minibatch Loss= 2.351658, Training Accuracy= 0.21875\n",
      "Iter 30, Minibatch Loss= 1.839725, Training Accuracy= 0.36719\n",
      "Iter 40, Minibatch Loss= 1.860830, Training Accuracy= 0.36719\n",
      "Iter 50, Minibatch Loss= 1.412946, Training Accuracy= 0.54297\n",
      "Iter 60, Minibatch Loss= 1.432029, Training Accuracy= 0.48828\n",
      "Iter 70, Minibatch Loss= 1.417948, Training Accuracy= 0.51953\n",
      "Iter 80, Minibatch Loss= 1.280374, Training Accuracy= 0.50000\n",
      "Iter 90, Minibatch Loss= 1.408058, Training Accuracy= 0.52344\n",
      "Iter 100, Minibatch Loss= 1.341776, Training Accuracy= 0.53906\n",
      "Iter 110, Minibatch Loss= 1.288788, Training Accuracy= 0.55859\n",
      "Iter 120, Minibatch Loss= 1.277626, Training Accuracy= 0.53125\n",
      "Iter 130, Minibatch Loss= 1.177666, Training Accuracy= 0.59766\n",
      "Iter 140, Minibatch Loss= 1.437111, Training Accuracy= 0.51172\n",
      "Iter 150, Minibatch Loss= 1.143404, Training Accuracy= 0.56641\n",
      "Iter 160, Minibatch Loss= 1.217747, Training Accuracy= 0.55859\n",
      "Iter 170, Minibatch Loss= 1.109308, Training Accuracy= 0.59766\n",
      "Iter 180, Minibatch Loss= 1.305265, Training Accuracy= 0.52344\n",
      "Iter 190, Minibatch Loss= 1.287480, Training Accuracy= 0.54688\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5894\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------                   \n",
    "# RBM을 쌓아 올려가며 DBN을 학습하는 텐서플로 그래프 실행   \n",
    "#-------------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables of the Model\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    n_batch = int(len(x_train)/batch_size)\n",
    "    # Start the training \n",
    "    for epoch in range(n_epoch):\n",
    "        # Loop over all batches\n",
    "        for i in range(n_batch):\n",
    "            batch_xs = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            # ++ 이미지 데이터를 784차원 벡터로 평탄화\n",
    "            batch_xs = batch_xs.reshape((-1, 784))\n",
    "            batch_xs = (batch_xs > 0)*1\n",
    "            # Run the weight update \n",
    "            _ = sess.run(updt, feed_dict={x:batch_xs})\n",
    "            \n",
    "        # Display the running step \n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1))\n",
    "                  \n",
    "    print(\"RBM training Completed !\")\n",
    "#--------------------------------------------------------------\n",
    "# 소프트맥스 층을 추가한 분류용-DBN 학습 및 예측\n",
    "#--------------------------------------------------------------\n",
    "    for i in range(n_iter):\n",
    "        indices = np.random.randint(len(x_train), size=batch_size)\n",
    "        batch_x = x_train[indices]\n",
    "        batch_y = y_train[indices]\n",
    "\n",
    "        # ++ 이미지 데이터를 784차원 벡터로 평탄화\n",
    "        batch_x = batch_x.reshape((-1, 784))\n",
    "        # ++ 레이블 데이터를 원-핫 인코딩\n",
    "        batch_y = np.eye(10)[batch_y]\n",
    "    \n",
    "        # 최적화 과정 실행\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if i % 10 == 0:\n",
    "            # MINIST 훈련용 이미지의 배치에 대한 손실과 정확도를 계산\n",
    "            tr_loss, tr_acc = sess.run([cost, accuracy], \n",
    "            \t                    feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(i) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(tr_loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(tr_acc))\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # MINIST 검정용 이미지에 대한 정확도 계산 \n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: x_test.reshape((-1, 784)), #타입 수정\n",
    "                                      y: np.eye(10)[y_test]})) #타입 수정\n",
    "\n",
    "    sess.close()\n",
    "    # *3-7 Testing Accuracy: 0.8769"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
